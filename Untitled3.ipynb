{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13447a6e-c05c-4e63-817d-3f49cc6fd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "english_df=pd.read_table(r\"C:\\Users\\Garv Khurana\\OneDrive\\Desktop\\NLLB.en-pa.en\",names=[\"english\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c684b53e-b0fd-4e34-90ec-b4e4fc41dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "punjabi_df=pd.read_table(r\"C:\\Users\\Garv Khurana\\OneDrive\\Desktop\\NLLB.en-pa.pa\", engine=\"python\", names=[\"punjabi\"],on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c893cd8-8e68-42fa-8084-16e6c34c158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(english_df) > len(punjabi_df):\n",
    "    punjabi_df = punjabi_df.reindex(english_df.index)\n",
    "else:\n",
    "    english_df = english_df.reindex(punjabi_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13e0b08-8002-4b83-9f6b-d90c3f107322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>punjabi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And Our Command is but a single (Act),- like t...</td>\n",
       "      <td>(ਅਤੇ) ਸਾਡਾ (ਕਿਸੇ ਪ੍ਰਕਾਰ ਦਾ ਕੋਈ) ਗੁਣ (ਜਾਂ) ਔਗੁਣ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He will be peaceful until the day when he dies!\"</td>\n",
       "      <td>(ਇਹ ਹਰਿ-ਨਾਮ) ਤੇਰੇ ਨਾਲ ਸਦਾ ਸਾਥ ਬਣਾਈ ਰੱਖੇਗਾ ॥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Behold, this is the Word that distinguishes (G...</td>\n",
       "      <td>(ਦੁੱਖਾਂ ਕਲੇਸ਼ਾਂ ਤੋਂ) ਬਚਣ ਦਾ ਇਹੀ ਪੱਕਾ ਵਸੀਲਾ ਹੈ ॥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But Iblis (did it not); he refused to be with ...</td>\n",
       "      <td>ਆਖ ਕਿ ਜਿਸ (ਜੀਵ ਇਸਤਰੀ) ਦੇ ਮੱਥੇ ਉਤੇ ਭਾਗ (ਲਿਖਿਆ ਹ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the condition of those who belied Our ...</td>\n",
       "      <td>ਪਰ ਅਜ਼ਰਾਈਲ ਨ ਸਾਰਾਂ, ਮੂਲੇ ਲੱਧੀਆਂ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749137</th>\n",
       "      <td>to form an LLC.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749138</th>\n",
       "      <td>triangles, head &amp; shoulders,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749139</th>\n",
       "      <td>where the person is resident in India</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749140</th>\n",
       "      <td>• Apprehend shoplifters in accordance with gui...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749141</th>\n",
       "      <td>• Cuts the consumption of chemicals by up to 50%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9749142 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "0        And Our Command is but a single (Act),- like t...   \n",
       "1         He will be peaceful until the day when he dies!\"   \n",
       "2        Behold, this is the Word that distinguishes (G...   \n",
       "3        But Iblis (did it not); he refused to be with ...   \n",
       "4        This is the condition of those who belied Our ...   \n",
       "...                                                    ...   \n",
       "9749137                                    to form an LLC.   \n",
       "9749138                       triangles, head & shoulders,   \n",
       "9749139              where the person is resident in India   \n",
       "9749140  • Apprehend shoplifters in accordance with gui...   \n",
       "9749141   • Cuts the consumption of chemicals by up to 50%   \n",
       "\n",
       "                                                   punjabi  \n",
       "0        (ਅਤੇ) ਸਾਡਾ (ਕਿਸੇ ਪ੍ਰਕਾਰ ਦਾ ਕੋਈ) ਗੁਣ (ਜਾਂ) ਔਗੁਣ...  \n",
       "1              (ਇਹ ਹਰਿ-ਨਾਮ) ਤੇਰੇ ਨਾਲ ਸਦਾ ਸਾਥ ਬਣਾਈ ਰੱਖੇਗਾ ॥  \n",
       "2          (ਦੁੱਖਾਂ ਕਲੇਸ਼ਾਂ ਤੋਂ) ਬਚਣ ਦਾ ਇਹੀ ਪੱਕਾ ਵਸੀਲਾ ਹੈ ॥  \n",
       "3        ਆਖ ਕਿ ਜਿਸ (ਜੀਵ ਇਸਤਰੀ) ਦੇ ਮੱਥੇ ਉਤੇ ਭਾਗ (ਲਿਖਿਆ ਹ...  \n",
       "4                         ਪਰ ਅਜ਼ਰਾਈਲ ਨ ਸਾਰਾਂ, ਮੂਲੇ ਲੱਧੀਆਂ।  \n",
       "...                                                    ...  \n",
       "9749137                                                NaN  \n",
       "9749138                                                NaN  \n",
       "9749139                                                NaN  \n",
       "9749140                                                NaN  \n",
       "9749141                                                NaN  \n",
       "\n",
       "[9749142 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([english_df, punjabi_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a5e345-a95e-488d-a0d0-7512dd0079b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   english  \\\n",
      "0        And Our Command is but a single (Act),- like t...   \n",
      "1         He will be peaceful until the day when he dies!\"   \n",
      "2        Behold, this is the Word that distinguishes (G...   \n",
      "3        But Iblis (did it not); he refused to be with ...   \n",
      "4        This is the condition of those who belied Our ...   \n",
      "...                                                    ...   \n",
      "8381220  Wikimedia Commons has media related to Univers...   \n",
      "8381221  Yamaha MT-03 (naked R3) to be launched soon in...   \n",
      "8381222                              Yes it is a priority.   \n",
      "8381223  You can also try doing a general search for th...   \n",
      "8381224  You can unsubscribe at any time, but you can n...   \n",
      "\n",
      "                                                   punjabi  \n",
      "0        (ਅਤੇ) ਸਾਡਾ (ਕਿਸੇ ਪ੍ਰਕਾਰ ਦਾ ਕੋਈ) ਗੁਣ (ਜਾਂ) ਔਗੁਣ...  \n",
      "1              (ਇਹ ਹਰਿ-ਨਾਮ) ਤੇਰੇ ਨਾਲ ਸਦਾ ਸਾਥ ਬਣਾਈ ਰੱਖੇਗਾ ॥  \n",
      "2          (ਦੁੱਖਾਂ ਕਲੇਸ਼ਾਂ ਤੋਂ) ਬਚਣ ਦਾ ਇਹੀ ਪੱਕਾ ਵਸੀਲਾ ਹੈ ॥  \n",
      "3        ਆਖ ਕਿ ਜਿਸ (ਜੀਵ ਇਸਤਰੀ) ਦੇ ਮੱਥੇ ਉਤੇ ਭਾਗ (ਲਿਖਿਆ ਹ...  \n",
      "4                         ਪਰ ਅਜ਼ਰਾਈਲ ਨ ਸਾਰਾਂ, ਮੂਲੇ ਲੱਧੀਆਂ।  \n",
      "...                                                    ...  \n",
      "8381220  ਵਿਦੇਸ਼ਾਂ ਵਿਚ ਵਸਦਿਆਂ ਤਾਂ ਮੇਰੀਆਂ ਪੰਜਾਬ ਫੇਰੀਆਂ ਲਗ...  \n",
      "8381221     ਨਸੀਬ ਸੋਚੀਂ ਪੈ ਗਈ ਕਿ ਇਹ ਸੱਭ ਕਿਵੇਂ ਹੱਲ ਹੋ ਸਕੇਗਾ?  \n",
      "8381222  ਇਨ੍ਹਾਂ ਨੂੰ ਦੇਖਣ ਵਾਲੇ ਯੂਨਾਨੀਆਂ ਨੇ ਇਨ੍ਹਾਂ ਦਾ ਵਰਣ...  \n",
      "8381223                 ਇਸ ਦੋ ਨੈਟਵਰਕਸ ਤੇ ਤੁਸੀਂ ਕਰ ਸਕਦੇ ਹੋਃ  \n",
      "8381224      ਅਤੇ ਸਮਝ ਆਉਣ ਤੇ \"ਸਮਾਂ\" ਬੀਤ ਗਿਆ ਹੁੰਦਾ ਹੈ. . . !  \n",
      "\n",
      "[8381225 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e75e900-ac28-4aa5-9577-6337a7b31d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>punjabi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And Our Command is but a single (Act),- like t...</td>\n",
       "      <td>(ਅਤੇ) ਸਾਡਾ (ਕਿਸੇ ਪ੍ਰਕਾਰ ਦਾ ਕੋਈ) ਗੁਣ (ਜਾਂ) ਔਗੁਣ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He will be peaceful until the day when he dies!\"</td>\n",
       "      <td>(ਇਹ ਹਰਿ-ਨਾਮ) ਤੇਰੇ ਨਾਲ ਸਦਾ ਸਾਥ ਬਣਾਈ ਰੱਖੇਗਾ ॥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Behold, this is the Word that distinguishes (G...</td>\n",
       "      <td>(ਦੁੱਖਾਂ ਕਲੇਸ਼ਾਂ ਤੋਂ) ਬਚਣ ਦਾ ਇਹੀ ਪੱਕਾ ਵਸੀਲਾ ਹੈ ॥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But Iblis (did it not); he refused to be with ...</td>\n",
       "      <td>ਆਖ ਕਿ ਜਿਸ (ਜੀਵ ਇਸਤਰੀ) ਦੇ ਮੱਥੇ ਉਤੇ ਭਾਗ (ਲਿਖਿਆ ਹ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the condition of those who belied Our ...</td>\n",
       "      <td>ਪਰ ਅਜ਼ਰਾਈਲ ਨ ਸਾਰਾਂ, ਮੂਲੇ ਲੱਧੀਆਂ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381220</th>\n",
       "      <td>Wikimedia Commons has media related to Univers...</td>\n",
       "      <td>ਵਿਦੇਸ਼ਾਂ ਵਿਚ ਵਸਦਿਆਂ ਤਾਂ ਮੇਰੀਆਂ ਪੰਜਾਬ ਫੇਰੀਆਂ ਲਗ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381221</th>\n",
       "      <td>Yamaha MT-03 (naked R3) to be launched soon in...</td>\n",
       "      <td>ਨਸੀਬ ਸੋਚੀਂ ਪੈ ਗਈ ਕਿ ਇਹ ਸੱਭ ਕਿਵੇਂ ਹੱਲ ਹੋ ਸਕੇਗਾ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381222</th>\n",
       "      <td>Yes it is a priority.</td>\n",
       "      <td>ਇਨ੍ਹਾਂ ਨੂੰ ਦੇਖਣ ਵਾਲੇ ਯੂਨਾਨੀਆਂ ਨੇ ਇਨ੍ਹਾਂ ਦਾ ਵਰਣ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381223</th>\n",
       "      <td>You can also try doing a general search for th...</td>\n",
       "      <td>ਇਸ ਦੋ ਨੈਟਵਰਕਸ ਤੇ ਤੁਸੀਂ ਕਰ ਸਕਦੇ ਹੋਃ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381224</th>\n",
       "      <td>You can unsubscribe at any time, but you can n...</td>\n",
       "      <td>ਅਤੇ ਸਮਝ ਆਉਣ ਤੇ \"ਸਮਾਂ\" ਬੀਤ ਗਿਆ ਹੁੰਦਾ ਹੈ. . . !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8381225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "0        And Our Command is but a single (Act),- like t...   \n",
       "1         He will be peaceful until the day when he dies!\"   \n",
       "2        Behold, this is the Word that distinguishes (G...   \n",
       "3        But Iblis (did it not); he refused to be with ...   \n",
       "4        This is the condition of those who belied Our ...   \n",
       "...                                                    ...   \n",
       "8381220  Wikimedia Commons has media related to Univers...   \n",
       "8381221  Yamaha MT-03 (naked R3) to be launched soon in...   \n",
       "8381222                              Yes it is a priority.   \n",
       "8381223  You can also try doing a general search for th...   \n",
       "8381224  You can unsubscribe at any time, but you can n...   \n",
       "\n",
       "                                                   punjabi  \n",
       "0        (ਅਤੇ) ਸਾਡਾ (ਕਿਸੇ ਪ੍ਰਕਾਰ ਦਾ ਕੋਈ) ਗੁਣ (ਜਾਂ) ਔਗੁਣ...  \n",
       "1              (ਇਹ ਹਰਿ-ਨਾਮ) ਤੇਰੇ ਨਾਲ ਸਦਾ ਸਾਥ ਬਣਾਈ ਰੱਖੇਗਾ ॥  \n",
       "2          (ਦੁੱਖਾਂ ਕਲੇਸ਼ਾਂ ਤੋਂ) ਬਚਣ ਦਾ ਇਹੀ ਪੱਕਾ ਵਸੀਲਾ ਹੈ ॥  \n",
       "3        ਆਖ ਕਿ ਜਿਸ (ਜੀਵ ਇਸਤਰੀ) ਦੇ ਮੱਥੇ ਉਤੇ ਭਾਗ (ਲਿਖਿਆ ਹ...  \n",
       "4                         ਪਰ ਅਜ਼ਰਾਈਲ ਨ ਸਾਰਾਂ, ਮੂਲੇ ਲੱਧੀਆਂ।  \n",
       "...                                                    ...  \n",
       "8381220  ਵਿਦੇਸ਼ਾਂ ਵਿਚ ਵਸਦਿਆਂ ਤਾਂ ਮੇਰੀਆਂ ਪੰਜਾਬ ਫੇਰੀਆਂ ਲਗ...  \n",
       "8381221     ਨਸੀਬ ਸੋਚੀਂ ਪੈ ਗਈ ਕਿ ਇਹ ਸੱਭ ਕਿਵੇਂ ਹੱਲ ਹੋ ਸਕੇਗਾ?  \n",
       "8381222  ਇਨ੍ਹਾਂ ਨੂੰ ਦੇਖਣ ਵਾਲੇ ਯੂਨਾਨੀਆਂ ਨੇ ਇਨ੍ਹਾਂ ਦਾ ਵਰਣ...  \n",
       "8381223                 ਇਸ ਦੋ ਨੈਟਵਰਕਸ ਤੇ ਤੁਸੀਂ ਕਰ ਸਕਦੇ ਹੋਃ  \n",
       "8381224      ਅਤੇ ਸਮਝ ਆਉਣ ਤੇ \"ਸਮਾਂ\" ਬੀਤ ਗਿਆ ਹੁੰਦਾ ਹੈ. . . !  \n",
       "\n",
       "[8381225 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3567ea3-9232-4384-ac94-590d0232790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"punjabi\"]=df[\"punjabi\"].apply(lambda x: f\"<start>{x}<end>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c0d3f6-a247-4c8b-bd30-3bed0151883d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english    0\n",
       "punjabi    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2016cf-3864-41bf-bb22-0419898e84bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc14569e-2890-4509-989b-d6db00b1bf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>punjabi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And Our Command is but a single (Act),- like t...</td>\n",
       "      <td>&lt;start&gt;(ਅਤੇ) ਸਾਡਾ (ਕਿਸੇ ਪ੍ਰਕਾਰ ਦਾ ਕੋਈ) ਗੁਣ (ਜਾ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He will be peaceful until the day when he dies!\"</td>\n",
       "      <td>&lt;start&gt;(ਇਹ ਹਰਿ-ਨਾਮ) ਤੇਰੇ ਨਾਲ ਸਦਾ ਸਾਥ ਬਣਾਈ ਰੱਖੇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Behold, this is the Word that distinguishes (G...</td>\n",
       "      <td>&lt;start&gt;(ਦੁੱਖਾਂ ਕਲੇਸ਼ਾਂ ਤੋਂ) ਬਚਣ ਦਾ ਇਹੀ ਪੱਕਾ ਵਸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But Iblis (did it not); he refused to be with ...</td>\n",
       "      <td>&lt;start&gt;ਆਖ ਕਿ ਜਿਸ (ਜੀਵ ਇਸਤਰੀ) ਦੇ ਮੱਥੇ ਉਤੇ ਭਾਗ (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the condition of those who belied Our ...</td>\n",
       "      <td>&lt;start&gt;ਪਰ ਅਜ਼ਰਾਈਲ ਨ ਸਾਰਾਂ, ਮੂਲੇ ਲੱਧੀਆਂ।&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381220</th>\n",
       "      <td>Wikimedia Commons has media related to Univers...</td>\n",
       "      <td>&lt;start&gt;ਵਿਦੇਸ਼ਾਂ ਵਿਚ ਵਸਦਿਆਂ ਤਾਂ ਮੇਰੀਆਂ ਪੰਜਾਬ ਫੇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381221</th>\n",
       "      <td>Yamaha MT-03 (naked R3) to be launched soon in...</td>\n",
       "      <td>&lt;start&gt;ਨਸੀਬ ਸੋਚੀਂ ਪੈ ਗਈ ਕਿ ਇਹ ਸੱਭ ਕਿਵੇਂ ਹੱਲ ਹੋ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381222</th>\n",
       "      <td>Yes it is a priority.</td>\n",
       "      <td>&lt;start&gt;ਇਨ੍ਹਾਂ ਨੂੰ ਦੇਖਣ ਵਾਲੇ ਯੂਨਾਨੀਆਂ ਨੇ ਇਨ੍ਹਾਂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381223</th>\n",
       "      <td>You can also try doing a general search for th...</td>\n",
       "      <td>&lt;start&gt;ਇਸ ਦੋ ਨੈਟਵਰਕਸ ਤੇ ਤੁਸੀਂ ਕਰ ਸਕਦੇ ਹੋਃ&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381224</th>\n",
       "      <td>You can unsubscribe at any time, but you can n...</td>\n",
       "      <td>&lt;start&gt;ਅਤੇ ਸਮਝ ਆਉਣ ਤੇ \"ਸਮਾਂ\" ਬੀਤ ਗਿਆ ਹੁੰਦਾ ਹੈ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8381212 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "0        And Our Command is but a single (Act),- like t...   \n",
       "1         He will be peaceful until the day when he dies!\"   \n",
       "2        Behold, this is the Word that distinguishes (G...   \n",
       "3        But Iblis (did it not); he refused to be with ...   \n",
       "4        This is the condition of those who belied Our ...   \n",
       "...                                                    ...   \n",
       "8381220  Wikimedia Commons has media related to Univers...   \n",
       "8381221  Yamaha MT-03 (naked R3) to be launched soon in...   \n",
       "8381222                              Yes it is a priority.   \n",
       "8381223  You can also try doing a general search for th...   \n",
       "8381224  You can unsubscribe at any time, but you can n...   \n",
       "\n",
       "                                                   punjabi  \n",
       "0        <start>(ਅਤੇ) ਸਾਡਾ (ਕਿਸੇ ਪ੍ਰਕਾਰ ਦਾ ਕੋਈ) ਗੁਣ (ਜਾ...  \n",
       "1        <start>(ਇਹ ਹਰਿ-ਨਾਮ) ਤੇਰੇ ਨਾਲ ਸਦਾ ਸਾਥ ਬਣਾਈ ਰੱਖੇ...  \n",
       "2        <start>(ਦੁੱਖਾਂ ਕਲੇਸ਼ਾਂ ਤੋਂ) ਬਚਣ ਦਾ ਇਹੀ ਪੱਕਾ ਵਸ...  \n",
       "3        <start>ਆਖ ਕਿ ਜਿਸ (ਜੀਵ ਇਸਤਰੀ) ਦੇ ਮੱਥੇ ਉਤੇ ਭਾਗ (...  \n",
       "4             <start>ਪਰ ਅਜ਼ਰਾਈਲ ਨ ਸਾਰਾਂ, ਮੂਲੇ ਲੱਧੀਆਂ।<end>  \n",
       "...                                                    ...  \n",
       "8381220  <start>ਵਿਦੇਸ਼ਾਂ ਵਿਚ ਵਸਦਿਆਂ ਤਾਂ ਮੇਰੀਆਂ ਪੰਜਾਬ ਫੇ...  \n",
       "8381221  <start>ਨਸੀਬ ਸੋਚੀਂ ਪੈ ਗਈ ਕਿ ਇਹ ਸੱਭ ਕਿਵੇਂ ਹੱਲ ਹੋ...  \n",
       "8381222  <start>ਇਨ੍ਹਾਂ ਨੂੰ ਦੇਖਣ ਵਾਲੇ ਯੂਨਾਨੀਆਂ ਨੇ ਇਨ੍ਹਾਂ...  \n",
       "8381223     <start>ਇਸ ਦੋ ਨੈਟਵਰਕਸ ਤੇ ਤੁਸੀਂ ਕਰ ਸਕਦੇ ਹੋਃ<end>  \n",
       "8381224  <start>ਅਤੇ ਸਮਝ ਆਉਣ ਤੇ \"ਸਮਾਂ\" ਬੀਤ ਗਿਆ ਹੁੰਦਾ ਹੈ....  \n",
       "\n",
       "[8381212 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c081f7df-57a6-4dda-96e1-2d9ed4850d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8381212, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe5ab2e-4323-45d0-86fb-1ee18f7dd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df.sample(n=155000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "794f3247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>punjabi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6683332</th>\n",
       "      <td>ceremony that includes the Blessing of Zaoth.</td>\n",
       "      <td>&lt;start&gt;'\" 32 ਯਿਸੂ ਨੇ ਆਖਿਆ, \"ਮੈਂ ਤੁਹਾਨੂੰ ਸੱਚ ਦੱ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187584</th>\n",
       "      <td>This maid of honour intentionally ruined her b...</td>\n",
       "      <td>&lt;start&gt;ਸ਼ਿਕਾਇਤ ਦੀ ਸਥਿਤੀ ਜਾਣੋ&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602875</th>\n",
       "      <td>- I told him the truth.</td>\n",
       "      <td>&lt;start&gt;ਜੇ ਤੇ ਵੱਟੇ ਦੇਣ ਮੁਰਾਦਾਂ, ਕਾਹਨੂੰ ਖ਼ਾਕ ਰੁਲ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3840014</th>\n",
       "      <td>Humpty Dumpty for president</td>\n",
       "      <td>&lt;start&gt;#ਸਤਵੀਰ ਨੇ ਗੁੱਟ ਤੋਂ ਫ਼ੜਕੇ ਖਿੱਚਿਆ ਤੇ ਸੀਨੇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891383</th>\n",
       "      <td>I confess on both hands.</td>\n",
       "      <td>&lt;start&gt;ਇਹ ਦੱਸਣਾ ਜ਼ਰੂਰੀ ਹੈ ਕਿ ਸਿਰਫ ਇਹੀ ਦੋ ਕੰਪਨੀ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "6683332      ceremony that includes the Blessing of Zaoth.   \n",
       "1187584  This maid of honour intentionally ruined her b...   \n",
       "4602875                            - I told him the truth.   \n",
       "3840014                        Humpty Dumpty for president   \n",
       "1891383                           I confess on both hands.   \n",
       "\n",
       "                                                   punjabi  \n",
       "6683332  <start>'\" 32 ਯਿਸੂ ਨੇ ਆਖਿਆ, \"ਮੈਂ ਤੁਹਾਨੂੰ ਸੱਚ ਦੱ...  \n",
       "1187584                  <start>ਸ਼ਿਕਾਇਤ ਦੀ ਸਥਿਤੀ ਜਾਣੋ<end>  \n",
       "4602875  <start>ਜੇ ਤੇ ਵੱਟੇ ਦੇਣ ਮੁਰਾਦਾਂ, ਕਾਹਨੂੰ ਖ਼ਾਕ ਰੁਲ...  \n",
       "3840014  <start>#ਸਤਵੀਰ ਨੇ ਗੁੱਟ ਤੋਂ ਫ਼ੜਕੇ ਖਿੱਚਿਆ ਤੇ ਸੀਨੇ...  \n",
       "1891383  <start>ਇਹ ਦੱਸਣਾ ਜ਼ਰੂਰੀ ਹੈ ਕਿ ਸਿਰਫ ਇਹੀ ਦੋ ਕੰਪਨੀ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82438cb3-744b-46e3-a530-0222027e1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(df_final[\"english\"])\n",
    "y=list(df_final[\"punjabi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f006e1-b36d-4173-8e04-96bd1ea3394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,TFAutoModelForTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicNER\")\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"ai4bharat/IndicNER\",from_pt=True)\n",
    "english_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75712115-e591-4f8b-9722-d63d85cc1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=50,test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6246f44e-dfcf-4244-b214-bce50be145b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=english_tokenizer(x_train,padding=\"max_length\",truncation=True,return_tensors=\"tf\",max_length=128)\n",
    "x_test=english_tokenizer(x_test,padding=\"max_length\",truncation=True,return_tensors=\"tf\",max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "289964e2-5815-44cb-9788-40dc04257fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=tokenizer(y_train,padding=\"max_length\",truncation=True,return_tensors=\"tf\",max_length=128)\n",
    "y_test=tokenizer(y_test,padding=\"max_length\",truncation=True,return_tensors=\"tf\",max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10647cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (93000, 128)\n",
      "Attention Mask shape: (93000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs shape:\",x_train['input_ids'].shape)\n",
    "print(\"Attention Mask shape:\", x_train['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7afd23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (62000, 128)\n",
      "Attention Mask shape: (62000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs shape:\",x_test['input_ids'].shape)\n",
    "print(\"Attention Mask shape:\", y_test['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20619203-4eb8-4705-ab77-64a7b0a28ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "training_dataset=tensorflow.data.Dataset.from_tensor_slices((dict(x_train),y_train)).batch(16)\n",
    "test_dataset=tensorflow.data.Dataset.from_tensor_slices((dict(x_test),y_test)).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "898862ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[ 101, 2023, 2003, ...,    0,    0,    0],\n",
      "       [ 101, 1038, 1012, ...,    0,    0,    0],\n",
      "       [ 101, 2053, 2017, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 1996, 2783, ...,    0,    0,    0],\n",
      "       [ 101, 2632, 2615, ...,    0,    0,    0],\n",
      "       [ 101, 2016, 1005, ...,    0,    0,    0]])>, 'token_type_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>}, {'input_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>})\n"
     ]
    }
   ],
   "source": [
    "for element in training_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e60a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[  101,  2026,  2564, ...,     0,     0,     0],\n",
      "       [  101,  2157,  1010, ...,     0,     0,     0],\n",
      "       [  101,  2320,  1010, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  2028,  2040, ...,     0,     0,     0],\n",
      "       [  101,  1998,  2044, ...,     0,     0,     0],\n",
      "       [  101,  2057, 12826, ...,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>}, {'input_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0],\n",
      "       [  101,   133, 13982, ...,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>})\n"
     ]
    }
   ],
   "source": [
    "for element in test_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a760d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in training_dataset.take(0):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a074fe52-6303-47d8-afa3-0c9ce806d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14b03188-5939-4cbb-bcb4-68d17fdc58f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  166765824 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  5383      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166771207 (636.18 MB)\n",
      "Trainable params: 166771207 (636.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05c4d678-f250-47f9-a83c-564b8b5803ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1706, in train_step  *\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 277, in __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 143, in __call__  *\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 270, in call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 128) and (None, 128, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_fileix546gsd.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_filehbz3g36k.py:45\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m     43\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mjit_compile, if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mnext\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(reduce_per_replica), (ag__\u001b[38;5;241m.\u001b[39mld(outputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdistribute_strategy), \u001b[38;5;28mdict\u001b[39m(reduction\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdistribute_reduction_method), fscope)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_filehbz3g36k.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     16\u001b[0m do_return_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m retval__1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcontrol_dependencies(ag__\u001b[38;5;241m.\u001b[39mld(_minimum_control_deps)(ag__\u001b[38;5;241m.\u001b[39mld(outputs))):\n\u001b[0;32m     20\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39m_train_counter\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_file1ry5sc7_.py:409\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;28;01mnonlocal\u001b[39;00m loss\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_23\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_23\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_24\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_24\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28mdict\u001b[39m(tape\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(tape)), fscope)\n\u001b[0;32m    411\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcompiled_metrics\u001b[38;5;241m.\u001b[39mupdate_state, (ag__\u001b[38;5;241m.\u001b[39mld(y), ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_file1ry5sc7_.py:404\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.if_body_23\u001b[1;34m()\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_23\u001b[39m():\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m loss\n\u001b[1;32m--> 404\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregularization_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_filed1lmzavl.py:203\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[1;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[0;32m    201\u001b[0m total_loss_mean_value \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss_mean_value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    202\u001b[0m loss_weight \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_weight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 203\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_9\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(y_t, y_p, sw, loss_obj, loss_weight, metric_obj)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_10\u001b[39m():\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (regularization_losses,)\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_filed1lmzavl.py:193\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m y_t, batch_dim, sw, y_p\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinue_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_p\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_filed1lmzavl.py:89\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body.<locals>.if_body_8\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m y_t, y_p, sw \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(match_dtype_and_rank), (ag__\u001b[38;5;241m.\u001b[39mld(y_t), ag__\u001b[38;5;241m.\u001b[39mld(y_p), ag__\u001b[38;5;241m.\u001b[39mld(sw)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     88\u001b[0m sw \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mapply_mask, (ag__\u001b[38;5;241m.\u001b[39mld(y_p), ag__\u001b[38;5;241m.\u001b[39mld(sw), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mget_mask, (ag__\u001b[38;5;241m.\u001b[39mld(y_p),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 89\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_p\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m total_loss_mean_value \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss_value)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_file__gxurzn.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     54\u001b[0m call_fn \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_fn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexecuting_eagerly, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_fn\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m in_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mget_mask, (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     58\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(losses_utils)\u001b[38;5;241m.\u001b[39mget_mask, (ag__\u001b[38;5;241m.\u001b[39mld(losses),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\GARVKH~1\\AppData\\Local\\Temp\\__autograph_generated_fileuowbtv4p.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(ag_fn), (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred)), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_fn_kwargs), fscope)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1706, in train_step  *\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 277, in __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 143, in __call__  *\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 270, in call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\Garv Khurana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 128) and (None, 128, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model.fit(training_dataset,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69762751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
